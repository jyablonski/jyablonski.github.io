<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="author" content="Jacob Yablonski">
    <meta name="description" content="Jacob Yablonski's Personal Blog">

    <title>Jacob Yablonski</title>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.97.8/css/materialize.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/font-awesome/3.1.1/css/font-awesome.css">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

    <link rel="icon" type="image/png" href="/static/pictures/favicon.ico">
    <link rel="stylesheet" type="text/css" href="/static/css/style.css">
    <link rel="stylesheet" type="text/css" href="/static/css/button.css">

    <script type="text/javascript" src="http://code.jquery.com/jquery-latest.js"></script>
    <script src="/static/js/webfont.js"></script>
    <script src="/static/js/snap.svg-min.js"></script>
    <script src="/static/js/underscore-min.js"></script>
    <script src="/static/js/sequence-diagram-min.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-HY68Y8REB4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-HY68Y8REB4');
    </script>
</head>
<body>
    <nav class="navbar">
      <div class="container">
        <a style="color:white;">Jacob Yablonski</a>
        <div style="float:right;">
          <ul style="list-style:none;display:flex;">
            <a href="/" class="left"><li class="link-list">Home</li></a>
            <a href="/posts/" class="left"><li class="link-list">Posts</li></a>
            <a href="/projects/" class="left"><li class="link-list">Projects</li></a>
            <a href="/resume/" class="left"><li class="link-list">Resume</li></a>
            <a href="/about/" class="left"><li class="link-list">About</li></a> 
          </ul>
        </div>
      </div>
    </nav>
    <div class="container" style="padding-bottom:100px;">
        <br>
        <main id="body-fade">
            
    <h2>Spark Notes</h2>
    <h1>Spark Entrypoint</h1>
<p><code>spark = SparkSession.builder.appName("Practice").getOrCreate()</code> is the entrypoint into Spark.  Before version 2.x.x, it was SparkContext or SQLContext or Hivecontext, but now SparkSession inherits all 3 for backwards compatibility.  You should only ever use <code>SparkSession</code>.</p>
<p>SparkContext establishes communication with the cluster and resource managers to coordinate and execute jobs.  Need to set an app name and then use getOrCreate().</p>
<p><code>import pyspark</code>
<code>from pyspark.sql import SparkSession</code>
<code>import pyspark.sql.functions as F</code> is a basic import that is used for a ton of shit.
spark.conf.set('spark.sql.repl.eagerEval.enabled', True) - will make it so your tables look good in jupyter notebooks</p>
<h1>Building your own data frames</h1>
<p>1) First way with tuples</p>
<p>test_df = spark_session.createDataFrame(
        data = [
            ('red', 'Jacob', 5),
            ('red', 'Jacob', 50),
            ('green', 'Jacob2', 20),
            ('black', 'Jacob3', 1000)
        ],
        schema = ['color', 'owner', 'price']
    )
allows you to build your own data, passing in lists for each column and then the schema</p>
<p>2) The other way is to do Row from pyspark</p>
<p>from pyspark.sql import Row
df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])</p>
<p>3) create it by defining StructField objects for each column (it's weird bc it's fucking java m8)</p>
<p>data_schema = [
               StructField('Car', StringType(), True),
               StructField('MPG', DoubleType(), True),
               StructField('Cylinders', IntegerType(), True),
               StructField('Displacement', DoubleType(), True),
               StructField('Horsepower', DoubleType(), True),
               StructField('Weight', DoubleType(), True),
               StructField('Acceleration', DoubleType(), True),
               StructField('Model', StringType(), True),
               StructField('Origin', StringType(), True),
            ]
final_struc = StructType(fields = data_schema)</p>
<p>data = spark.read.csv(
    'cars.csv',
    sep = ';',
    header = True,
    schema = final_struc 
    )</p>
<p>4) </p>
<p>df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])
df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col0"])</p>
<h1>AWS S3 Connection</h1>
<p>The Below code will automatically download the necessary shit to connect.
I had to download the latest possible versions of whatever from <a href="https://hadoop.apache.org/docs/r2.8.0/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers">here</a> to get it to work.  -- apparently YOUR version of pyspark needs to match with the hadoop aws version you download.
The access credentials need to be for a IAM user which has S3 Read Access (and write access if you want to write files)</p>
<p>conf = SparkConf()
conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1') # basically most recent version as my spark version
conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')
conf.set('spark.hadoop.fs.s3a.access.key', os.environ.get('aws_access_key_id'))
conf.set('spark.hadoop.fs.s3a.secret.key', os.environ.get('aws_secret_access_key'))</p>
<p>spark = SparkSession.builder.config(conf=conf).getOrCreate()</p>
<p>and then you can do</p>
<p>df = spark.read.csv('s3a://jacobsbucket97/sample_files/nba_tweets.csv', inferSchema = True, header = True)
df.createTempView('nba_data')
df_s3_spark = spark.sql("SELECT * FROM nba_data where language = 'en' limit 5;")</p>
<h1>PostgreSQL Connection</h1>
<p>sparkClassPath = os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.1.1 pyspark-shell'
spark = SparkSession.builder.appName("Practice").config("spark.driver.extraClassPath", sparkClassPath).getOrCreate()</p>
<p>df = spark.read \
    .format("jdbc") \
    .option("url", f"jdbc:postgresql://{os.environ.get('IP')}/{os.environ.get('RDS_DB')}") \
    .option("driver", "org.postgresql.Driver") \
    .option("dbtable", f"{os.environ.get('RDS_SCHEMA')}.aws_odds_source") \
    .option("user", os.environ.get('RDS_USER')) \
    .option("password", os.environ.get('RDS_PW')) \
    .load()</p>
<p>You have to use jdbc and then put in all of your shit.  Note that <code>dbtable</code> option actually has to have the schema appeneded to it first (myschema.mytable)</p>
<h1>Reading and Writing Files</h1>
<p>parquetDF = spark.read.parquet("/tmp/databricks-df-example.parquet")
parquetDF.write.parquet("databricks-df-example.parquet")</p>
<p>df = spark.read.option('header', 'true').csv('nba_tweets.csv')
df = spark.read.csv('cars.csv', header=True, sep=";")
df = spark.read.csv('cars.csv', header=True, sep=";", inferSchema=True) # use this when loading in dfs</p>
<h1>DataFrame Data Munging</h1>
<p>This takes a input df and filters the <code>color</code> column to values of <code>red</code>, and then it groups by the <code>owner</code> column and aggregates the <code>price</code> column into a new column called <code>grouped_price</code>.
    * To filter on a string you have to use F.lit('str')
    * To aggregate you have to do F.sum('price_column')
    * To do a case when you have to do F.when(F.col('my_col'))</p>
<p>The next dataframe then creates a case when statement for a new column that filters that <code>grouped_price</code> column and values of &gt; 10 get populated with 1, while others are just 0 in a variable called <code>indicator</code>.  It then filters this column for values of 1.</p>
<p>inter_df = input_df.where(input_df['color'] == \
                        F.lit('red')).groupBy('owner').agg(F.sum('price').alias('grouped_price'))</p>
<p>output_df = inter_df.select('owner', 'grouped_price', \
                            F.when(F.col('grouped_price') &gt; 10, 1).otherwise(0).alias('indicator')).where(
            F.col('indicator') == F.lit(1))</p>
<h1>DataFrame Column Renaming</h1>
<p>To rename columns do</p>
<p>df = df.withColumnRenamed('first_column', 'new_column_one') \
       .withColumnRenamed('second_column', 'new_column_two') \
       .withColumnRenamed('third_column', 'new_column_three')</p>
<h1>DataFrame Count / Group by</h1>
<p>df.groupBy('Origin').count().show(5)</p>
<h1>DataFrame Column deletion</h1>
<p>df = df.drop('new_column_two') \
       .drop('new_column_three')</p>
<h1>DataFrame Filtering</h1>
<p>europe_filtered_count = df.filter(col('Origin')=='Europe').count()</p>
<p>df.filter(df.a == 1).show()</p>
<h1>DataFrame distinct rows</h1>
<p>df.select('Origin').distinct().show()
df.select('Origin','model').distinct().show()</p>
<h1>Collecting pyspark data</h1>
<p><code>df.collect()</code> will put it into memory (i think as a list?)
<code>df.take(5)</code> will only take a certain number of records
<code>df.toPandas()</code> will turn it back into a Pandas Dataframe</p>
<p>df2 = df.collect()
df2 = df.take(5)
df_pandas = df.toPandas()</p>
<h1>UDFs</h1>
<p>from pyspark.sql.functions import pandas_udf</p>
<p>@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -&gt; pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1</p>
<p>df.select(pandas_plus_one(df.a)).show()</p>

        </main>
    </div>
    <nav class="navfoot">
      <div style="text-align:center;">
        &copy; 2022 |
        &thinsp; <a href="https://www.linkedin.com/in/jacobyablonski/" class="link-fa"><i class="fa fa-linkedin fa-lg"></i></a>
        &thinsp; <a href="https://github.com/jyablonski" class="link-fa"><i class="fa fa-github fa-lg"></i></a>
        &thinsp; <a href="mailto:jyablonski9@gmail.com" class="link-fa"><i class="fa fa-envelope fa-lg"></i></a>
      </div>
    </nav>
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
</body>

</html>